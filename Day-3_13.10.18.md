---
title: 'Day-3: 13 Oct, 18'
author: "tiangsinf"
date: "10/13/2018"
output: 
  html_document:
    keep_md: yes
---
### Multiple Linear Regression



```r
library(tidyverse)
library(ggvis)
```


```r
# simple linear model from day-2.
simple_linear <- lm(mpg ~ wt, mtcars)
summary(simple_linear)
```

```
## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  37.2851     1.8776  19.858  < 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528,	Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10
```
*p-value* for `wt` is very close to 0 indicating strong predicting power on `mpg`.


```r
# Multiple linear regression
str(mtcars)
```

```
## 'data.frame':	32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
```

```r
# Define formula
f3 <- as.formula(paste(
  "mpg ~ ",
  paste(
    names(mtcars[c(2, 3, 6)]),
    collapse = " + "
  )
))

# Set regression
m_regression <- lm(formula = f3,
                   data = mtcars)
summary(m_regression)
```

```
## 
## Call:
## lm(formula = f3, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.4035 -1.4028 -0.4955  1.3387  6.0722 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 41.107678   2.842426  14.462 1.62e-14 ***
## cyl         -1.784944   0.607110  -2.940  0.00651 ** 
## disp         0.007473   0.011845   0.631  0.53322    
## wt          -3.635677   1.040138  -3.495  0.00160 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.595 on 28 degrees of freedom
## Multiple R-squared:  0.8326,	Adjusted R-squared:  0.8147 
## F-statistic: 46.42 on 3 and 28 DF,  p-value: 5.399e-11
```
#### P-Value
We can see that `disp` has *p-value* of 0.53 which is way more than the desire (below) 0.05. We can safely conclude that `disp` has no predictive power on `mpg`.

#### R-Square (R2)
Compared to the **adjusted R2** from simple linear, the difference between R2 and adjusted R2 is bigger in the muliple regression model. This is because the adjusted R2 penalise excessive use of variables.

#### F-Statistics
The *F-stats* is another guideline to example varialbe significants. Looking at the multiple regression model, the F-stats (46.42) is significantly lower than the simple linear regression F-stats (91.38). Given (if) the overall p-value is still < 0.05, we can assumpt that our new model have lower predictive power because of lower F-stats.


```r
# Define formula
f2 <- as.formula(paste(
  "mpg ~ ",
  paste(
    names(mtcars[c(2, 6)]),
    collapse = " + "
  )
))

# Set regression
m_regression <- lm(formula = f2,
                   data = mtcars)
summary(m_regression)
```

```
## 
## Call:
## lm(formula = f2, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2893 -1.5512 -0.4684  1.5743  6.1004 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  39.6863     1.7150  23.141  < 2e-16 ***
## cyl          -1.5078     0.4147  -3.636 0.001064 ** 
## wt           -3.1910     0.7569  -4.216 0.000222 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.568 on 29 degrees of freedom
## Multiple R-squared:  0.8302,	Adjusted R-squared:  0.8185 
## F-statistic: 70.91 on 2 and 29 DF,  p-value: 6.809e-12
```
Dropping disp from the model improves predictive power of the model. The difference between adjusted-R2 and R2 is narrower and the p-value is even smaller!

### Regression Assumptions:
1.  Linearity
2.  No endogeneity (https://en.wikipedia.org/wiki/Endogeneity_(econometrics) (London house price vs size example)
3.  Normality and homoscedacity (https://en.wikipedia.org/wiki/Homoscedasticity)
4.  No autocolleration (use Durbin-Watson test)
    Result of DW test falls between 0 and 4 where:
    * 2 indicates no autocorrelation
    * <1 and >3 cause an alarm

```r
library(lmtest)
```
    

```r
dwtest(m_regression) # dw result is 1.67
```

```
## 
## 	Durbin-Watson test
## 
## data:  m_regression
## DW = 1.6711, p-value = 0.1335
## alternative hypothesis: true autocorrelation is greater than 0
```
5.  No multicolinearity



